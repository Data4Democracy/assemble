{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wwymak/anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pymongo\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mongoClient = MongoClient()\n",
    "db = mongoClient.data4democracy\n",
    "tweets_collection = db.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_model = Word2Vec.load_word2vec_format('../../../../Volumes/SDExternal2/word2vec_twitter_model/word2vec_twitter_model.bin', binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jewish', 0.6926181316375732),\n",
       " ('hispanic', 0.6038353443145752),\n",
       " ('muslim', 0.5737464427947998),\n",
       " ('armenian', 0.5712549686431885),\n",
       " ('iranian', 0.5708979368209839),\n",
       " ('mormon', 0.567450761795044),\n",
       " ('mexican', 0.5669983625411987),\n",
       " ('protestant', 0.5593392252922058),\n",
       " ('Chaldean', 0.5580775737762451),\n",
       " ('asian', 0.5575110912322998)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now calculate word simiarities on twitter data e.g.  \n",
    "tweets_model.most_similar('jewish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to remind myself what a tweet is like:\n",
    "r = requests.get('https://s3-us-west-2.amazonaws.com/discursive/2017/1/10/18/tweets-25.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': \"I Fuck Up... Just don't forget you Fuck Up Too.\", 'original_name': 'Linda Suhler, Ph.D.', 'created': '2017-01-10 18:14:08', 'id_str': '818883641640177665', 'name': 'VFL2013', 'loc': None, 'retweet': 'Y', 'text': \"RT @LindaSuhler: Can we hear from #MSM here?\\n@MTV's @Ira Madison III Calls Jeff Sessions' Granddaughter 'Prop' Stolen from Toys R Us… \", 'original_id': 347627434, 'followers': 3098, 'hashtags': '[\"MSM\"]', 'user_created': '2012-12-29 17:54:08', 'friends_count': 979, 'retweet_count': 0}\n"
     ]
    }
   ],
   "source": [
    "tweets_collection = r.json()\n",
    "print(tweets_collection[0])\n",
    "#for text analysis, the 'text' field is the one of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @LindaSuhler: Can we hear from #MSM here?\n",
      "@MTV's @Ira Madison III Calls Jeff Sessions' Granddaughter 'Prop' Stolen from Toys R Us… \n"
     ]
    }
   ],
   "source": [
    "#the tweets text are in the 'text' field\n",
    "print(tweets_collection[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a bit of experimentation/learning with gensim -- following along some tutuorials on the gensim site to vectorize text, find tfidf etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_text_documents = [x['text'] for x in tweets_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @LindaSuhler: Can we hear from #MSM here?\\n@MTV's @Ira Madison III Calls Jeff Sessions' Granddaughter 'Prop' Stolen from Toys R Us… \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick check that the mapping was done correctly\n",
    "tweets_text_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'lindasuhler',\n",
       " 'hear',\n",
       " 'msm',\n",
       " 'mtv',\n",
       " 'ira',\n",
       " 'madison',\n",
       " 'iii',\n",
       " 'calls',\n",
       " 'jeff',\n",
       " 'sessions',\n",
       " 'granddaughter',\n",
       " 'prop',\n",
       " 'stolen',\n",
       " 'toys']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick check of the tokenize function -- remove stopwords included \n",
    "tokenize(tweets_text_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_tweets = [[word for word in tokenize(x) if word != 'rt'] for x in tweets_text_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lindasuhler',\n",
       " 'hear',\n",
       " 'msm',\n",
       " 'mtv',\n",
       " 'ira',\n",
       " 'madison',\n",
       " 'iii',\n",
       " 'calls',\n",
       " 'jeff',\n",
       " 'sessions',\n",
       " 'granddaughter',\n",
       " 'prop',\n",
       " 'stolen',\n",
       " 'toys']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#construct a dictoinary of the words in the tweets using gensim\n",
    "# the dictionary is a mapping between words and their ids\n",
    "tweets_dictionary = corpora.Dictionary(tokenized_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save gyhe dict for future reference\n",
    "tweets_dictionary.save('temp/tweets_dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agend': 453,\n",
       " 'aware': 865,\n",
       " 'big': 1273,\n",
       " 'coming': 908,\n",
       " 'declare': 1042,\n",
       " 'derekf': 575,\n",
       " 'est': 1671,\n",
       " 'hximdj': 570,\n",
       " 'jb': 1127,\n",
       " 'nabs': 1056,\n",
       " 'nationalists': 321,\n",
       " 'plan': 596,\n",
       " 'qx': 880,\n",
       " 'rw': 1347,\n",
       " 'suspect': 1069,\n",
       " 'thought': 752,\n",
       " 'tlot': 378,\n",
       " 'tries': 185,\n",
       " 'vikingriver': 1448,\n",
       " 'wdiemokb': 1504}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just a quick view of words and ids\n",
    "dict(list(tweets_dictionary.token2id.items())[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert tokenized documents to vectors\n",
    "# compile corpus (vectors number of times each elements appears)\n",
    "tweet_corpus = [tweets_dictionary.doc2bow(x) for x in tokenized_tweets]\n",
    "corpora.MmCorpus.serialize('temp/tweets_corpus.mm', tweet_corpus) # save for future ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_tfidf_model = gensim.models.TfidfModel(tweet_corpus, id2word = tweets_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x2a3b6bc88>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tfidf_model[tweet_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create similarity matrix of all tweets\n",
    "'''note from gensim docs: The class similarities.MatrixSimilarity is only appropriate when \n",
    "   the whole set of vectors fits into memory. For example, a corpus of one million documents \n",
    "   would require 2GB of RAM in a 256-dimensional LSI space, when used with this class.\n",
    "   Without 2GB of free RAM, you would need to use the similarities.Similarity class.\n",
    "   This class operates in fixed memory, by splitting the index across multiple files on disk, \n",
    "   called shards. It uses similarities.MatrixSimilarity and similarities.SparseMatrixSimilarity internally,\n",
    "   so it is still fast, although slightly more complex.'''\n",
    "index = similarities.MatrixSimilarity(tweets_tfidf_model[tweet_corpus]) \n",
    "index.save('temp/tweetsSimilarity.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500)\n"
     ]
    }
   ],
   "source": [
    "#get similarity matrix between docs: https://groups.google.com/forum/#!topic/gensim/itYEaOYnlEA\n",
    "#and check that the similarity matrix is what you expect\n",
    "tweets_similarity_matrix = np.array(index)\n",
    "print(tweets_similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the similarity matrix and associated tweets to json\n",
    "#work in progress-- use tSNE to visualise the tweets to see if there's any clustering\n",
    "outputDict = {'tweets' : [{'text': x['text'], 'id': x['id_str'], 'user': x['original_name']} for x in tweets_collection], 'matrix': tweets_similarity_matrix.tolist()}\n",
    "with open('temp/tweetSimilarity.json', 'w') as f:\n",
    "    json.dump(outputDict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#back to the word2vec idea, use min_count since corpus is tiny\n",
    "tweets_collected_model = gensim.models.Word2Vec(tokenized_tweets, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blocked', 0.3967747688293457),\n",
       " ('wall', 0.3779504895210266),\n",
       " ('blackanddecker', 0.36672842502593994),\n",
       " ('white', 0.33267900347709656),\n",
       " ('campaign', 0.3270014524459839),\n",
       " ('product', 0.3262655735015869),\n",
       " ('jail', 0.32027339935302734),\n",
       " ('nytimes', 0.3143633008003235),\n",
       " ('maga', 0.3098933696746826),\n",
       " ('community', 0.30887705087661743)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_collected_model.most_similar('jewish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
