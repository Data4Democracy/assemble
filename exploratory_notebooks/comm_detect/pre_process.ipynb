{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using JSON, ProgressMeter, JLD, LightGraphs, MatrixMarket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Graph building and analysis\n",
    "The goal is to build the retweet graph using julia and look at centrality measures to identify the most central nodes.\n",
    "Also this should be useful to the NMF work in the other notbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Helper Function Definitions\n",
    "At some point I should move this to a jl file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirwalk (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a stand in for python's os.walk.\n",
    "# will apply the function fn to whatever file is at path, then close the file\n",
    "function dirwalk(path::AbstractString, fn::Function)\n",
    "  content = readdir(path)\n",
    "  for c in content\n",
    "    p = joinpath(path, c)\n",
    "    if isdir(p)\n",
    "      dirwalk(p, fn)\n",
    "    elseif isfile(p)\n",
    "        println(p)\n",
    "        open(fn, p)\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_data_to_dict (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary where key is the name of the tweeter and values is an array\n",
    "# of strings containing the people the tweeter retweeted\n",
    "const di = Dict{String,Array{String,1}}()\n",
    "\n",
    "function add_data_to_dict(f::IOStream)\n",
    "    lines = readlines(f)\n",
    "    try\n",
    "    tweets = JSON.parse(lines[1])\n",
    "    for tweet in tweets\n",
    "        if tweet[\"retweet\"] != \"N\"\n",
    "            if haskey(di, tweet[\"name\"])\n",
    "                push!(di[tweet[\"name\"]], tweet[\"original_name\"])\n",
    "            else\n",
    "                di[tweet[\"name\"]] = [tweet[\"original_name\"]]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fill_data (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this converts the data from di \n",
    "# into a list of names and a name_to_index dictionary that we \n",
    "# can use to build the graph\n",
    "\n",
    "function fill_data(di::Dict{String,Array{String,1}})\n",
    "    name_to_index = Dict{String, Int64}()\n",
    "    names = Array{String}(0)\n",
    "    for (k, vs) in di\n",
    "            push!(names, k)\n",
    "        for v in vs\n",
    "            push!(names, v)\n",
    "        end\n",
    "    end\n",
    "    names = unique(names)\n",
    "    \n",
    "    for (i, n) in enumerate(names)\n",
    "        name_to_index[n] = i\n",
    "    end\n",
    "    return names, name_to_index\n",
    "end\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes our data format (a dictionary of string -> [string] and a dictionary of string -> int)\n",
    "# and builds the LightGraph by adding all nodes and edges\n",
    "\n",
    "function build_graph(graph_dict::Dict{String, Array{String, 1}},\n",
    "                    name_to_index::Dict{String, Int64})\n",
    "    graph = LightGraphs.Graph(length(names))\n",
    "    for (key, val) in graph_dict\n",
    "        @showprogress for item in val\n",
    "            if item != \"CC\" # @CCs in tweets need to be removed\n",
    "                source = name_to_index[key]\n",
    "                target = name_to_index[item]\n",
    "                add_edge!(graph, source, target)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return graph\n",
    "end\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function fill_nontweeters(m::SparseMatrixCSC{Int64, Int64},\n",
    "    non_tweeter_transfo, n_lines::Int64, n_cols::Int64)::SparseMatrixCSC{Int64, Int64}\n",
    "    is, js, vals = findnz(m)\n",
    "    new_is = Array{Int64,1}()\n",
    "    new_js = Array{Int64,1}()\n",
    "    new_vals = Array{Int64,1}()\n",
    "    \n",
    "    for i in 1:length(is)\n",
    "        if haskey(non_tweeter_transfo, is[i] )\n",
    "            push!(new_is, non_tweeter_transfo[is[i]])\n",
    "            push!(new_js, js[i])\n",
    "            push!(new_vals, vals[i])\n",
    "        end\n",
    "    end\n",
    "    sparse(new_is, new_js, new_vals, n_lines, n_cols)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function fill_reduced(m::SparseMatrixCSC{Int64, Int64},\n",
    "    corpus_indices, n_lines::Int64, n_cols::Int64)::SparseMatrixCSC{Int64, Int64}\n",
    "    is,js,vals = findnz(m)\n",
    "    sparse([corpus_indices[i] for i in is], js, vals, n_lines, n_cols)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the retweet graph \n",
    "First, create the dictionnary from the json files then save it to a jld file for later use\n",
    "At this point, this probably should be moved to the parse_py side of things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filling out the dictionary\n",
    "dirwalk(\"/media/henripal/hd1/data/\", add_data_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Array{String,1}} with 1076038 entries:\n",
       "  \"Deborah87958167\" => String[\"texasfarmgirl1836\",\"texasfarmgirl1836\",\"Squatch\"…\n",
       "  \"AdolescentIdle\"  => String[\"ㅤㅤㅤ\"]\n",
       "  \"DCSlove1129\"     => String[\"キャロット🥕\",\"🏳️\\u200d🌈みねりんど dsr🕊\",\"Vane…\n",
       "  \"kwirick\"         => String[\"Carol Tilley\",\"Jade @ Katsucon F12!\"]\n",
       "  \"lisa_ventriss\"   => String[\"Jason Finley\",\"Jason Finley\"]\n",
       "  \"IQueenParrilla\"  => String[\"Dorothy Lydia\"]\n",
       "  \"Mia_Cluer\"       => String[\"TOBY STEPHENS\"]\n",
       "  \"Pankaj7073\"      => String[\"Planned Parenthood\",\"Alma Har'el\",\"Huffington Po…\n",
       "  \"mdufay\"          => String[\"chicago lacy\",\"chicago lacy\",\"David Burge\",\"Stev…\n",
       "  \"ndominic22\"      => String[\"🇺🇸Cris 🇺🇸\"]\n",
       "  \"AM3009\"          => String[\"The Anti-Trump\"]\n",
       "  \"dbdj1007\"        => String[\"#BRITVIDZAYN\",\"Taylor Swift\",\"Farhad Manjoo\"]\n",
       "  \"fgbabcock1\"      => String[\"Charles P. Pierce\"]\n",
       "  \"drjohnhayes\"     => String[\"Donald Trump Jr.\",\"Donald Trump Jr.\"]\n",
       "  \"juan_2085\"       => String[\"CC\"]\n",
       "  \"tumblinginto\"    => String[\"Maksim Chmerkovskiy\",\"Maksim Chmerkovskiy\"]\n",
       "  \"danipaich\"       => String[\"Jack Grimes\"]\n",
       "  \"hegem0n\"         => String[\"TruthFeed News\",\"James O'Keefe\",\"Ali\",\"Shoahed S…\n",
       "  \"genny_dlxon\"     => String[\"Mark A. Skoda\",\"🎙Wayne Dupree\",\"🎙Wayne Dupree\"…\n",
       "  \"LiamsAdv3ntures\" => String[\"Ayton FC Sunday Res\",\"Ayton FC Sunday Res\",\"Ayto…\n",
       "  \"andrewm2805\"     => String[\"Ewan McGregor\"]\n",
       "  \"slwalter123\"     => String[\"Midnight Circus\",\"NPR Books\"]\n",
       "  \"MarcusBeaubier\"  => String[\"crystal\",\"Rainbow Rowell\",\"CNN\",\"shauna\",\"Tommy …\n",
       "  \"2simplyb\"        => String[\"CNN\"]\n",
       "  \"endribacuku2\"    => String[\"CC\",\"CC\"]\n",
       "  ⋮                 => ⋮"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serialization options for later\n",
    "# save(\"/media/henripal/hd1/data/temp.jld\", \"di\", di)\n",
    "# di = JLD.load(\"/media/henripal/hd1/data/temp.jld\", \"di\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1076038"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to be able to build a graph with only integers (we need the graph structure to be lightweight), but still be able to get the main info. We create an array of unique names and a dictionary linking the unique name to the index in the graph.\n",
    "Note that the `name` field in the data is not the best as it does not reflect the twitter user's handle, can contain unicode and be hard to work with, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names, name_to_index = fill_data(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more serialization\n",
    "#JLD.save(\"/media/henripal/hd1/data/names.jld\", \"names\", names)\n",
    "#JLD.save(\"/media/henripal/hd1/data/name_to_index.jld\", \"name_to_index\", name_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#names = JLD.load(\"/media/henripal/hd1/data/names.jld\",\"names\")\n",
    "#name_to_index = JLD.load(\"/media/henripal/hd1/data/name_to_index.jld\", \"name_to_index\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fill the edges according to the retweet structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: Lightraphs not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Lightraphs not defined",
      ""
     ]
    }
   ],
   "source": [
    "graph = build_graph(di, name_to_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JLD.save(\"/media/henripal/hd1/data/graph.jld\", \"graph\", graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the user/word matrix from the python data\n",
    "Now that we have our graph, remains to build the user/word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the sparse bow matrix from the python notebook (thanks @wwymak)\n",
    "corpus = MatrixMarket.mmread(\"/media/henripal/hd1/data/corp.mm\")\n",
    "corpus = convert(SparseMatrixCSC{Int64, Int64}, corpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the names from the tweets extracted from the python notebook\n",
    "corpus_names= readdlm(\"/media/henripal/hd1/data/corpus_names.csv\",',',String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cleaning required to make the python dictionary talk to the julia dictionary. We need to:\n",
    "- identify the subset of names that are tweeters (they index the corpus)\n",
    "- identify the subset of names that are retweeted, but not tweeters (these are nodes of the graph, not in the corpus)\n",
    "Out total number of nodes in the graph is the union of these two sets. We need to create a word vector for the second set; since they are being retweeted, we attribute to them the words of the retweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_lines = length(names)\n",
    "n_cols = size(corpus)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_indices = [name_to_index[corpus_name] for corpus_name in corpus_names]\n",
    "corpus_indices_set = Set(corpus_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we build the reverse dictionary: retweeted -> tweeter. \n",
    "# this will be used later to build the words associated with the retweeter\n",
    "\n",
    "rev_di = Dict{String, String}()\n",
    "for (tweeter,retweeteds) in di\n",
    "    for retweeted in retweeteds\n",
    "        rev_di[retweeted] = tweeter\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here non_tweeters are the subset of indices in the graph, not in the corpus.\n",
    "# some list comprehension fun to build the mapping that will map them to their words\n",
    "\n",
    "non_tweeter_indices = [i for i in 1:length(names) if ~ (i in corpus_indices_set)];\n",
    "non_tweeter_names = [names[i] for i in non_tweeter_indices];\n",
    "non_tweeter_transfo = Dict(zip([name_to_index[rev_di[n]] for n in non_tweeter_names], non_tweeter_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the user_word matrix, with only the common names (other lines show up as zeros)\n",
    "user_word = fill_reduced(corpus, corpus_indices, n_lines, n_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finally, we add the nontweeters, add the two matrices, and we are done!\n",
    "\n",
    "nontweeters_word = fill_nontweeters(user_word, non_tweeter_transfo, n_lines, n_cols)\n",
    "user_word = user_word + nontweeters_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JLD.save(\"/media/henripal/hd1/data/user_word.jld\", \"user_word\", user_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some preliminary indicators for the giant component\n",
    "Maybe hack this part out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a limited size dataset where connectivity is indicated by retweets, many nodes will be isolated. Happily, graph theory still tells us that we should get good coverage using only the largest connected or giant component, so we will now create that subgraph and check that it has a good size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting the connected components and sorting them by their size\n",
    "conn = connected_components(graph)\n",
    "sort!(conn, by = length, rev=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1067329, 3114809} undirected graph"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gettin the giant component\n",
    "giant, giant_nodes = induced_subgraph(graph, conn[1])\n",
    "giant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{120, 119} undirected graph"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now the size of the second largets connected component:\n",
    "induced_subgraph(graph, conn[2])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, the second largest connected component is mini. To check that we find good stuff, we'll name the 10 most connected nodes and see if they make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centrality = degree_centrality(giant);\n",
    "centrality_tuples = collect(zip(centrality, giant_nodes)); \n",
    "sort!(centrality_tuples, by = x -> x[1], rev = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women's March\n",
      "Ewan McGregor\n",
      "Lori Hendry\n",
      "Donald J. Trump\n",
      "James Woods\n",
      "Michael Nöthem\n",
      "Sandraن\n",
      "Linda Suhler, Ph.D.\n",
      "Donald Trump Jr.\n",
      "Patriotic Rosie\n",
      "Ivanka Trump\n",
      "Carmine Zozzora\n",
      "Lou Dobbs\n",
      "Trump We Trust\n",
      "Brian Fraser\n",
      "Scott Dworkin\n",
      "Immigrants☆4☆Trump\n",
      "霧月\n",
      "Trump Inauguration\n",
      "John K Stahl\n"
     ]
    }
   ],
   "source": [
    "for i in 1:20\n",
    "    println(names[centrality_tuples[i][2]])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
