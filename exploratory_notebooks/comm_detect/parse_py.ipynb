{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and cleaning tweets\n",
    "This notebook is a slight modification of @wwymak's word2vec notebook, with different tokenization, and a way to iterate over tweets linked to their named user "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WWmyak's iterator and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import pymoji\n",
    "import importlib\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim import corpora\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from six import iteritems\n",
    "import csv\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def keep_retweets(tweets_objs_arr):\n",
    "    return [x[\"text\"] for x in tweets_objs_arr if x['retweet'] != 'N'], [x[\"name\"] for x in tweets_objs_arr if x['retweet'] != 'N'], [x[\"followers\"] for x in tweets_objs_arr if x['retweet'] != 'N']\n",
    "\n",
    "def convert_emojis(tweets_arr):\n",
    "    return [pymoji.replaceEmojiAlt(x, trailingSpaces=1) for x in tweets_arr]\n",
    "\n",
    "def tokenize_tweets(tweets_arr):\n",
    "    result = []\n",
    "    for x in tweets_arr:\n",
    "        try:\n",
    "            tokenized = tokenizer.tokenize(x)\n",
    "            result.append([x.lower() for x in tokenized if x not in string.punctuation])\n",
    "        except:\n",
    "            pass\n",
    "#             print(x)\n",
    "    return result\n",
    "\n",
    "class Tweets(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for root, directories, filenames in os.walk(self.dirname):\n",
    "            for filename in filenames:\n",
    "                if(filename.endswith('json')):\n",
    "                    print(root + filename)\n",
    "                    with open(os.path.join(root,filename), 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                        data_parsed_step1, user_names, followers = keep_retweets(data)\n",
    "                        data_parsed_step2 = convert_emojis(data_parsed_step1)\n",
    "                        data_parsed_step3 = tokenize_tweets(data_parsed_step2)\n",
    "                        for data, name, follower in zip(data_parsed_step3, user_names, followers):\n",
    "                            yield name, data, follower\n",
    "\n",
    "\n",
    "#model = gensim.models.Word2Vec(sentences, workers=2, window=5, sg = 1, size = 100, max_vocab_size = 2 * 10000000)\n",
    "#model.save('tweets_word2vec_2017_1_size100_window5')\n",
    "#print('done')\n",
    "#print(time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My gensim tinkering\n",
    "Tasks:\n",
    "- build the gensim dictionary\n",
    "- build the bow matrix using this dictionary (sparse matrix so memory friendly)\n",
    "- save the names and the dicitionary for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# building the dictionary first, from the iterator\n",
    "sentences = Tweets('/media/henripal/hd1/data/2017/1/') # a memory-friendly iterator\n",
    "dictionary = corpora.Dictionary((tweet for _, tweet, _ in sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here we use the downloaded  stopwords from nltk and create the list\n",
    "# of stop ids using the hash defined above\n",
    "stop = set(stopwords.words('english'))\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stop if stopword in dictionary.token2id]\n",
    "\n",
    "# and this is the items we don't want - that appear less than 20 times\n",
    "# hardcoded numbers FTW\n",
    "low_freq_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq  <1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finally we filter the dictionary and compactify\n",
    "dictionary.filter_tokens(stop_ids + low_freq_ids)\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reinitializing the iterator to get more stuff\n",
    "sentences = Tweets('/media/henripal/hd1/data/2017/1/')\n",
    "corpus = []\n",
    "name_to_follower = {}\n",
    "names = []\n",
    "\n",
    "for name, tweet, follower in sentences:\n",
    "    corpus.append(tweet) \n",
    "    names.append(name)\n",
    "    name_to_follower[name] = follower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we save everything for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/media/henripal/hd1/data/name_to_follower.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in name_to_follower.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "with open('/media/henripal/hd1/dta/corpus_names.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we save the sparse bow corpus matrix using matrix market format\n",
    "corpora.MmCorpus.serialize('/media/henripal/hd1/data/corp.mm', corpus)\n",
    "\n",
    "# and we save the dictionary as a text file\n",
    "dictionary.save('/media/henripal/hd1/data/dict')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
