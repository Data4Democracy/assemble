{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using JSON, ProgressMeter, JLD, LightGraphs, TextAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Graph building and analysis\n",
    "The goal is to build the retweet graph using julia and look at centrality measures to identify the most central nodes.\n",
    "Also this should be useful to the NMF work in the other notbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Helper Function Definitions\n",
    "At some point I should move this to a jl file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirwalk (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a stand in for python's os.walk.\n",
    "# will apply the function fn to whatever file is at path, then close the file\n",
    "function dirwalk(path::AbstractString, fn::Function)\n",
    "  content = readdir(path)\n",
    "  for c in content\n",
    "    p = joinpath(path, c)\n",
    "    if isdir(p)\n",
    "      dirwalk(p, fn)\n",
    "    elseif isfile(p)\n",
    "        println(p)\n",
    "        open(fn, p)\n",
    "    end\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_data_to_dict (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary where key is the name of the tweeter and values is an array\n",
    "# of strings containing the people the tweeter retweeted\n",
    "const di = Dict{String,Array{String,1}}()\n",
    "\n",
    "function add_data_to_dict(f::IOStream)\n",
    "    lines = readlines(f)\n",
    "    try\n",
    "    tweets = JSON.parse(lines[1])\n",
    "    for tweet in tweets\n",
    "        if tweet[\"retweet\"] != \"N\"\n",
    "            if haskey(di, tweet[\"name\"])\n",
    "                push!(di[tweet[\"name\"]], tweet[\"original_name\"])\n",
    "            else\n",
    "                di[tweet[\"name\"]] = [tweet[\"original_name\"]]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fill_data (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this converts the data from di \n",
    "# into a list of names and a name_to_index dictionary that we \n",
    "# can use to build the graph\n",
    "\n",
    "function fill_data(di::Dict{String,Array{String,1}})\n",
    "    name_to_index = Dict{String, Int64}()\n",
    "    names = Array{String}(0)\n",
    "    for (k, vs) in di\n",
    "            push!(names, k)\n",
    "        for v in vs\n",
    "            push!(names, v)\n",
    "        end\n",
    "    end\n",
    "    names = unique(names)\n",
    "    \n",
    "    for (i, n) in enumerate(names)\n",
    "        name_to_index[n] = i\n",
    "    end\n",
    "    return names, name_to_index\n",
    "end\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the entire dataset\n",
    "We can now extend the analysis to the entire dataset.\n",
    "First, create the dictionnary from the json files then save it to a jld file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirwalk(\"/media/henripal/hd1/data/\", add_data_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialization options for later\n",
    "# save(\"/media/henripal/hd1/data/temp.jld\", \"di\", di)\n",
    "# di = JLD.load(\"/media/henripal/hd1/data/temp.jld\", \"di\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1076038"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to be able to build a graph with only integers (we need the graph structure to be lightweight), but still be able to get the main info. We create an array of unique names and a dictionary linking the unique name to the index in the graph.\n",
    "Note that the `name` field in the data is not the best as it does not reflect the twitter user's handle, can contain unicode and be hard to work with, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(String[\"Deborah87958167\",\"texasfarmgirl1836\",\"Squatch\",\"Lu Who\",\"SongsOfLaredo\",\"Diva\",\"Bishop Talbert Swan\",\"NadelParis\",\"Buster Brown\",\"AdolescentIdle\"  …  \"leeanndroid\",\"chabudai0001\",\"SisSissaki\",\"QKout\",\"roa_isa\",\"HolyFuzazzle\",\"SofiaGuapura\",\"JohnJulia18\",\"KPniele\",\"Name Redacted\"],Dict(\"Deborah87958167\"=>1,\"AdolescentIdle\"=>10,\"DCSlove1129\"=>12,\"kwirick\"=>16,\"lisa_ventriss\"=>19,\"IQueenParrilla\"=>21,\"Shaun O'Banion\"=>451587,\"Mia_Cluer\"=>23,\"Pankaj7073\"=>25,\"mdufay\"=>32…))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names, name_to_index = fill_data(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more serialization\n",
    "JLD.save(\"/media/henripal/hd1/data/names.jld\", \"names\", names)\n",
    "JLD.save(\"/media/henripal/hd1/data/name_to_index.jld\", \"name_to_index\", name_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1205559, 0} undirected graph"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph(length(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fill the edges according to the retweet structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100%|█████████████████████████████████████████| Time: 0:00:01\n",
      "Progress: 100%|█████████████████████████████████████████| Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "for (key, val) in di\n",
    "    @showprogress for item in val\n",
    "        if item != \"CC\" # @CCs in tweets need to be removed\n",
    "            source = name_to_index[key]\n",
    "            target = name_to_index[item]\n",
    "            add_edge!(graph, source, target)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a limited size dataset where connectivity is indicated by retweets, many nodes will be isolated. Happily, graph theory still tells us that we should get good coverage using only the largest connected or giant component, so we will now create that subgraph and check that it has a good size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting the connected components and sorting them by their size\n",
    "conn = connected_components(graph)\n",
    "sort!(conn, by = length, rev=true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1067329, 3114809} undirected graph"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gettin the giant component\n",
    "giant, giant_nodes = induced_subgraph(graph, conn[1])\n",
    "giant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{120, 119} undirected graph"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now the size of the second largets connected component:\n",
    "induced_subgraph(graph, conn[2])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, the second largest connected component is mini. To check that we find good stuff, we'll name the 10 most connected nodes and see if they make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centrality = degree_centrality(giant);\n",
    "centrality_tuples = collect(zip(centrality, giant_nodes)); \n",
    "sort!(centrality_tuples, by = x -> x[1], rev = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women's March\n",
      "Ewan McGregor\n",
      "Lori Hendry\n",
      "Donald J. Trump\n",
      "James Woods\n",
      "Michael Nöthem\n",
      "Sandraن\n",
      "Linda Suhler, Ph.D.\n",
      "Donald Trump Jr.\n",
      "Patriotic Rosie\n",
      "Ivanka Trump\n",
      "Carmine Zozzora\n",
      "Lou Dobbs\n",
      "Trump We Trust\n",
      "Brian Fraser\n",
      "Scott Dworkin\n",
      "Immigrants☆4☆Trump\n",
      "霧月\n",
      "Trump Inauguration\n",
      "John K Stahl\n"
     ]
    }
   ],
   "source": [
    "for i in 1:20\n",
    "    println(names[centrality_tuples[i][2]])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection using Non-negative matrix factorization\n",
    "Note that the first step here would be to sign the edges according to the fact that some edges are endorsements or not. However for prototyping we will skip this step as we do not have the edited tweet info. I think this is available in the full twitter API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step - build the user-word matrix\n",
    "This is the main objective in the DualNMF presented in the [paper](https://arxiv.org/abs/1608.01771). We will ignore connected words and words that show up less than 20 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## see SparseMatrix.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
