{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communities in Twitter\n",
    "\n",
    "\n",
    "* Adapted from [Mastering Social Media Mining with Python](https://www.packtpub.com/big-data-and-business-intelligence/mastering-social-media-mining-python).\n",
    "\n",
    "* For more information look at [Intro_Collecting_tweets](https://github.com/Data4Democracy/assemble/blob/master/tutorials/Intro_Collecting_Tweets.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "\n",
    "from tweepy import API\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "from tweepy import Cursor\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key    = 'xxxxxxxxxxxxxxxxxxxx'\n",
    "consumer_secret = 'xxxxxxxxxxxxxxxxxxxx' \n",
    "access_token    = 'xxxxxxxxxxxxxxxxxxxx'\n",
    "access_secret   = 'xxxxxxxxxxxxxxxxxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twitter_auth():\n",
    "    \"\"\"Setup Twitter Authentication.\n",
    "    \n",
    "    Return: tweepy.OAuthHandler object\n",
    "    \"\"\"\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return auth\n",
    "    \n",
    "def get_twitter_client():\n",
    "    \"\"\"Setup Twitter API Client.\n",
    "    \n",
    "    Return: tweepy.API object\n",
    "    \"\"\"\n",
    "    auth = get_twitter_auth()\n",
    "    client = API(auth)\n",
    "    return client\n",
    "\n",
    "client = get_twitter_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data\n",
    "\n",
    "## Input Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "screen_name = 'X1alejandro3x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paginate(items, n):\n",
    "    \"\"\"Generate n-sized chunks for items.\"\"\"\n",
    "    for i in range(0, len(items), n):\n",
    "        yield items[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make directory\n",
    "dirname = 'users/{}'.format(screen_name)\n",
    "try:\n",
    "    os.makedirs(dirname, mode=0o755, exist_ok=True)\n",
    "except OSError:\n",
    "    print('Directory {} already exists.'.format(dirname))\n",
    "    \n",
    "# Max num of requests per window\n",
    "MAX_FRIENDS = 15000\n",
    "max_pages = math.ceil(MAX_FRIENDS / 5000)\n",
    "    \n",
    "# get followers for a given user\n",
    "fname = 'users/{}/followers.jsonl'.format(screen_name)\n",
    "with open(fname, 'w') as f:\n",
    "    for followers in Cursor(client.followers_ids, screen_name=screen_name).pages(max_pages):\n",
    "        for chunk in paginate(followers, 100):\n",
    "            users = client.lookup_users(user_ids=chunk)\n",
    "            for user in users:\n",
    "                f.write(json.dumps(user._json)+'\\n')\n",
    "        if len(followers) == 5000:\n",
    "            print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `max_df` : Features appearing in more than than `max_df`% of the documents will be ignored (i.e., stop words).\n",
    "* `min_df` : Features that occur in less than `min_df` documnets will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (1387, 200)\n",
      "---------------------------------------------------- Cluster 0\n",
      "Data scientist & cinephile – I tweet mainly about #DataViz  – Creator of https://t.co/9bjdRdRrCz & https://t.co/2bqanRlP9q\n",
      "AllSight is the first Customer Intelligence Management system that manages and synthesizes ALL data to provide the 'big picture' for your customer.\n",
      "Leading data-driven business\n",
      "Data scientist (@adobe) by day, electronics enthusiast by night, and physicist at heart.\n",
      "Data Scientist/Machine Learning Engineer\n",
      "Student. Love data #MachineLearning #DataScience #InfoSec #AI\n",
      "Data Scientist @docenthealth; Alumni @TelecomPTech; Writing a book on Amazon Machine Learning for @PacktPub; #NLProc #Algorithms #MachineLearning\n",
      "Data Science Software Development, Social Media, Digital Marketing, Predictive Analytics, Machine Learning, AI, PCA, R, Python, Hadoop, Spark, Tableau, NLP\n",
      "Extracting Actionable Intelligence from Data - The Only True Benchmarking Opportunity For Heads\n",
      "Of Data Insights From Europe's Most Progressive Companies\n",
      "Research scientist | Trying to spend more time outdoors\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------- Cluster 1\n",
      "SEO for Growth: The Ultimate Guide for Marketers, Web Designers & Entrepreneurs, a search engine optimization book by John Jantsch & Phil Singleton.\n",
      "How #DataVisualization Drives Business Impact\n",
      "Turn any #software into a #cloud solution\n",
      "\n",
      "Transforme les logiciels #legacy en solution #Cloud #SaaS sans redéveloppement.\n",
      "Our Mission is to promote food safety by increasing awareness of food borne illness & formation of partnerships within the food industry.\n",
      "perdida entre páginas de libros, y ahogada en una taza de café .\n",
      "CEO @ubisend. A chatbot and AI messaging platform to help companies attract readers, drive leads and close sales #startup #conversationalmarketing #ai #chatbots\n",
      "Chatbot Developer. Passion for using machine learning and artificial intelligence to solve complex problems. Chat with us: https://t.co/QSTMJwaTTF\n",
      "Verified reviews and personalised recommendations of the best places to eat.\n",
      "\n",
      "https://t.co/6cyRQOyFHs\n",
      "World's #1 tech job site\n",
      "Bringing to you all the latest in the world of business intelligence #BI #BigData\n",
      "\n",
      "\n",
      "\n",
      "---------------------------------------------------- Cluster 2\n",
      "Software engineer, Traveller, Entrepreneur, Science enthusiast\n",
      "The current state of actual science and mathematics--not the popularized stuff. Join us.\n",
      "Scientist in NASA's Earth Sciences Division, AAAS STP Fellow. Science Advocate - Specializing in Ecology, Evolution, Remote Sensing, and Geography\n",
      "Complexity science nerd, pattern interruptor. #IBMer. @NECSI affiliate. Founding Editor, contributor @d4emergence. Views my own.\n",
      "Mathematician-Economics-Statistics-Computer Science-Triathlon-Quantative-Finance\n",
      "Theoretical physicist. Science blogger. Amateur chess and go player. Professional chocolate eater.\n",
      "Physicist pretending to be a chemist, sometimes the other way around. University lecturer doing research in Computational Materials Science.\n",
      "Lifelong student and lover of science. Most recently, a dog mom and absolutely loving it!\n",
      "BA Political Science May 2000 MBAE February 2014 Medical Scientist Training Program Candidate for MD/PhD 2014. Fluent in French and Spanish, German, Japanese.\n",
      "Interested in #science, #GPCR and drug discovery. Any expressed opinion is solely my own.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 3                     # Number of clusters\n",
    "max_features = 200        # Max number of features\n",
    "max_ngram = 4             # Upper boundary for ngrams to be extracted\n",
    "\n",
    "max_df = 0.8              # Max document freq for a feature \n",
    "min_df = 2                # Min document freq for a feature \n",
    "min_ngram = 1             # Lower boundary for ngrams to be extracted\n",
    "use_idf = True            # True==TF-IDF, False==TF\n",
    "\n",
    "with open(fname) as f:\n",
    "    users = []\n",
    "    for line in f:\n",
    "        profile = json.loads(line)\n",
    "        users.append(profile['description'])\n",
    "        \n",
    "    vectorizer = TfidfVectorizer(max_df=max_df,\n",
    "                                 min_df=min_df,\n",
    "                                 max_features=max_features,\n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(min_ngram, max_ngram),\n",
    "                                 use_idf=use_idf)\n",
    "    X = vectorizer.fit_transform(users)\n",
    "    print('Data dimensions: {}'.format(X.shape))\n",
    "    \n",
    "    # perform clustering\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X)\n",
    "    clusters = defaultdict(list)\n",
    "    for i, label in enumerate(km.labels_):\n",
    "        clusters[label].append(users[i])\n",
    "        \n",
    "    for label, descriptions in clusters.items():\n",
    "        print('---------------------------------------------------- Cluster {}'.format(label))\n",
    "        for desc in descriptions[:10]:\n",
    "            print(desc)\n",
    "        print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
