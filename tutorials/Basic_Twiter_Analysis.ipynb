{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twiter Analysis\n",
    "\n",
    "* For more information look at [Intro_Collecting_tweets](https://github.com/Data4Democracy/assemble/blob/master/tutorials/Intro_Collecting_Tweets.ipynb)\n",
    "\n",
    "* This notebook is inspired from [Marco Bonzanini](https://marcobonzanini.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    \"\"\"Get all hastags from a Tweet.\n",
    "    \n",
    "    Instead of using tweet['entities'], for example,\n",
    "    we use get() which will not raise a KeyError if\n",
    "    said field is not present.\n",
    "    \n",
    "    Return: list(hashtags)\n",
    "    \"\"\"\n",
    "    entities = tweet.get('entities', {})\n",
    "    hastags = entities.get('hashtags', [])\n",
    "    return [hashtag['text'].lower() for hashtag in hastags]\n",
    "\n",
    "def get_mentions(tweet):\n",
    "    \"\"\"Get all user mentions from a Tweet.\n",
    "        \n",
    "    Return: list(mentions)\n",
    "    \"\"\"\n",
    "    entities = tweet.get('entities', {})\n",
    "    mentions = entities.get('user_mentions', [])\n",
    "    return [mention['screen_name'] for mention in mentions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This was obtained using the procedure showed in `intro_collecting_tweets` notebook\n",
    "\n",
    "fname = 'usr_timeline_kdnuggets.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 3200 tweets total, 0.950 % of the tweets had at least 1 hashtag.\n"
     ]
    }
   ],
   "source": [
    "with open(fname, 'r') as f:\n",
    "    hashtagCount = defaultdict(int)\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweetHashtags = get_hashtags(tweet)\n",
    "        numHashtags = len(tweetHashtags)\n",
    "        \n",
    "        hashtagCount[numHashtags] += 1\n",
    "    \n",
    "    tweets_Hashtags = sum([count for numTags, count in hashtagCount.items() if numTags>0])\n",
    "    tweets_noHashtags = hashtagCount[0]\n",
    "    tweetsTotal = tweets_Hashtags + tweets_noHashtags\n",
    "    \n",
    "    print('Out of {0} tweets total, {1:.3f} % of the tweets had at least 1 hashtag.'.format(\n",
    "            tweetsTotal, tweets_Hashtags/tweetsTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 3200 tweets total, 0.222 % of the tweets had at least 1 mention.\n"
     ]
    }
   ],
   "source": [
    "with open(fname, 'r') as f:\n",
    "    mentionCount = defaultdict(int)\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweetmentions = get_mentions(tweet)\n",
    "        nummentions = len(tweetmentions)\n",
    "        \n",
    "        mentionCount[nummentions] += 1\n",
    "    \n",
    "    tweets_Mentions = sum([count for numMentions, count in mentionCount.items() if numMentions>0])\n",
    "    tweets_noMentions = mentionCount[0]\n",
    "    tweetsTotal = tweets_Mentions + tweets_noMentions\n",
    "    \n",
    "    print('Out of {0} tweets total, {1:.3f} % of the tweets had at least 1 mention.'.format(\n",
    "            tweetsTotal, tweets_Mentions/tweetsTotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Mentions and Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1242   ---   kdn                 \n",
      "  842   ---   datascience         \n",
      "  696   ---   machinelearning     \n",
      "  445   ---   icymi               \n",
      "  408   ---   deeplearning        \n",
      "  368   ---   bigdata             \n",
      "  207   ---   analytics           \n",
      "  187   ---   ai                  \n",
      "  165   ---   python              \n",
      "  136   ---   datascientist       \n"
     ]
    }
   ],
   "source": [
    "with open(fname, 'r') as f:\n",
    "    hashtags = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweetHashtags = get_hashtags(tweet)\n",
    "        hashtags.update(tweetHashtags)\n",
    "\n",
    "for hashtag, count in hashtags.most_common(10):\n",
    "    print('{0:5}   ---   {1:20}'.format(count, hashtag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39   ---   kdnuggets           \n",
      "   28   ---   KDnuggetsJobs       \n",
      "   21   ---   mattmayo13          \n",
      "   17   ---   odsc                \n",
      "   14   ---   OReillyMedia        \n",
      "   13   ---   DJ44                \n",
      "   12   ---   bigdataconf         \n",
      "   12   ---   Microsoft           \n",
      "   11   ---   DeepMindAI          \n",
      "   11   ---   jameskobielus       \n"
     ]
    }
   ],
   "source": [
    "with open(fname, 'r') as f:\n",
    "    mentions = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tweetmentions = get_mentions(tweet)\n",
    "        mentions.update(tweetmentions)\n",
    "\n",
    "for mention, count in mentions.most_common(10):\n",
    "    print('{0:5}   ---   {1:20}'.format(count, mention))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(text, tokenizer=TweetTokenizer(), stopwords=[]):\n",
    "    \"\"\"Process the text of a tweet:\n",
    "    -Lowercase\n",
    "    -Tokenize\n",
    "    -Stopword removal\n",
    "    -Digits removal\n",
    "    \n",
    "    Return: list(strings)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = normalize_contractions(tokens)\n",
    "    return [token for token in tokens if token not in stopwords and not token.isdigit()]\n",
    "\n",
    "def normalize_contractions(tokens):\n",
    "    \"\"\"Normalize english contractions.\n",
    "    \n",
    "    Return: generator\n",
    "    \"\"\"\n",
    "    token_map = {\n",
    "        \"i'm\" : \"i am\",\n",
    "        \"you're\" : \"you are\",\n",
    "        \"it's\" : \"it is\",\n",
    "        \"we'll\" : \"we will\",\n",
    "    }\n",
    "    for token in tokens:\n",
    "        if token in token_map.keys():\n",
    "            for item in token_map[token].split():\n",
    "                yield item\n",
    "        else:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'usr_timeline_kdnuggets.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1242   ---   #kdn                \n",
      "  842   ---   #datascience        \n",
      "  696   ---   #machinelearning    \n",
      "  445   ---   #icymi              \n",
      "  408   ---   #deeplearning       \n",
      "  368   ---   #bigdata            \n",
      "  339   ---   top                 \n",
      "  274   ---   data                \n",
      "  207   ---   #analytics          \n",
      "  187   ---   #ai                 \n",
      "  165   ---   #python             \n",
      "  136   ---   #datascientist      \n",
      "  135   ---   kdnuggets           \n",
      "  124   ---   #datamining         \n",
      "  120   ---   #neuralnetworks     \n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = stopwords.words('english') + punct + ['rt', 'via', '...', 'â€¦']\n",
    "\n",
    "twit = Counter()\n",
    "with open(fname, 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = process(text=tweet['text'],\n",
    "                         tokenizer=tweet_tokenizer,\n",
    "                         stopwords=stopword_list)\n",
    "        twit.update(tokens)\n",
    "        \n",
    "for token, count in twit.most_common(15):\n",
    "    print('{0:5}   ---   {1:20}'.format(count, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
